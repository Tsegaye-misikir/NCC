{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwECcSZvLLCB"
   },
   "source": [
    "## Final Tests and Scores\n",
    "\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5qKfxEEE9E"
   },
   "source": [
    "**General Information**\n",
    "- Inside this Notebook, different methods to create multilingual document representations are tested and evaluated. \n",
    "\n",
    "- Which methods, parameter-settings and languages are used for the evaluation can be adjusted by changing the variables in the Cell below. \n",
    "\n",
    "- This Notebook was run in Google Colab. \n",
    "\n",
    "**About the Methods and Datasets**\n",
    "\n",
    "\n",
    "\n",
    "Methods:\n",
    "\n",
    "- Methods which are based on creating mappings between monolingual corpora.\n",
    "Those methods are: Linear Concept Approximation (LCA), Linear Concept Compression(LCC) and the Neural Network versions of those: NNCA and NNCC. \n",
    "For them, first the monolingual representation have to be created, then the mapping can be applied. Algorithms which are applied here to derive monolingual representations are: Latent Semantic indexing and Doc2Vec (Mikolov et al.)\n",
    "\n",
    "- Methods which derive multilingual representations directly. Those are: Cross-Lingual Latent Semantic Indexing (CL-LSI) and the improved version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hshTz7z4DH1y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----\n",
    "Languages Preprocessed for JRC_Arquis: en, hu, fr, de, nl, pt, cz, pl\n",
    "Languages Preprocessed for EU-Bookshop: en, es, fr\n",
    "\n",
    "\"\"\"\n",
    "#Choose either \"JRC_Arquis\" \"EU-Bookshop\"\n",
    "dataset =\"EU-Bookshop\"\n",
    "\n",
    "#Determines which methods are tested\n",
    "# True -> Method is evaluated\n",
    "# False -> Method is ignored\n",
    "test_LCA = False\n",
    "test_LCC = False\n",
    "test_CLLSI = False\n",
    "test_neural_networks = True\n",
    "\n",
    "#Set languages, dimensions and kind of monolingual embedding\n",
    "#The monolingual embedding method influences the results of \n",
    "# LCA, LCC, NNCA, and NNCC\n",
    "languages = [\"en\", \"es\", \"fr\"] #[\"en\", \"hu\", \"fr\", \"de\", \"nl\", \"pt\", \"cs\", \"pl\"]\n",
    "embedding_method = \"LSI\"\n",
    "\n",
    "\n",
    "#BEST PARAMETERS/PARAMETERS TO BE TESTED\n",
    "lca_dimension = [500]\n",
    "lcao_dimension =[500]\n",
    "lcc_dimension = [500]\n",
    "cllsi_dimension = [500]\n",
    "settings_nncc = [     \n",
    "             ]\n",
    "\n",
    "settings_nnca = [  \n",
    "            {\"dimension\" : 500,\n",
    "             \"neurons\" : [500], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbdMEV93vg7D"
   },
   "outputs": [],
   "source": [
    "all_dimensions = lca_dimension + lcao_dimension + lcc_dimension\n",
    "dimensions = list(dict.fromkeys(all_dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It0niRYTn2Y1"
   },
   "source": [
    "##  Load Dataset\n",
    "- First of all, clone the git repository which contains most of the functions and models for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnPUdnM5n4_H",
    "outputId": "593ef7f1-0e31-41bf-9293-6b3072a4bccb"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Tsegaye-misikir/NCC.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK3ve7knzwr5"
   },
   "source": [
    "- then load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elxdSt42S5Yt",
    "outputId": "4a8b7a11-4c02-4c7e-a396-19568955dc67"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "if dataset == \"JRC_Arquis\" :\n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/JRC_Arquis_files/\"\n",
    "  sample_df = pd.read_pickle(main_dir+\"sample_df_preprocessed.pkl\")\n",
    "  train_df = sample_df[:3000]\n",
    "  val_df = sample_df[3000:4000]\n",
    "  test_df = sample_df[4000:5000]\n",
    "  \n",
    "elif dataset == \"EU-Bookshop\": \n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/EU-BookShop Files/\"\n",
    "  #define\n",
    "\n",
    "  def get_eub_dataframe(main_dir):\n",
    "    def load(filepath):\n",
    "      with open(filepath,\"rb\") as f:\n",
    "          obj = pickle.load(f)\n",
    "      return obj\n",
    "    tokenized_en = load(main_dir+\"/tokenized_en.list\")\n",
    "    tokenized_fr = load(main_dir+\"/tokenized_fr.list\")\n",
    "    tokenized_es = load(main_dir+\"/tokenized_es.list\")\n",
    "    sample_df = pd.DataFrame()\n",
    "    sample_df[\"body_pre_en\"] = tokenized_en\n",
    "    sample_df[\"body_pre_fr\"] = tokenized_fr\n",
    "    sample_df[\"body_pre_es\"] = tokenized_es\n",
    "    #erase empty lists\n",
    "    for key in sample_df.keys():\n",
    "      sample_df = sample_df[sample_df.astype(str)[key] != '[]']\n",
    "    return sample_df\n",
    "\n",
    "  sample_df = get_eub_dataframe(main_dir)[:5000]\n",
    "  train_df = sample_df[:3000]\n",
    "  val_df = sample_df[3000:4000]\n",
    "  test_df = sample_df[4000:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev7mVFQMvFly"
   },
   "source": [
    "## Train Monolingual Representations which will be aligned\n",
    "- > Define the languages and dimensions which should be tested here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6lpKqf-yIUG"
   },
   "outputs": [],
   "source": [
    "from NCC.Utils import read_docs, Vector_Lsi_Model\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "\n",
    "max_dim = max(dimensions)\n",
    "matrices = dict()\n",
    "\n",
    "\n",
    "if embedding_method == \"LSI\":\n",
    "  lsi_models = dict()\n",
    "  for t in languages:\n",
    "    key = \"body_pre_{}\".format(t)\n",
    "    lsi_models[t] = Vector_Lsi_Model(sample_df[key], dimension=max_dim)\n",
    "    matrices[\"{}_train_vecs\".format(t)] = np.asarray(lsi_models[t].create_embeddings(train_df[key]))\n",
    "    matrices[\"{}_val_vecs\".format(t)] = np.asarray(lsi_models[t].create_embeddings(val_df[key]))\n",
    "    matrices[\"{}_test_vecs\".format(t)] = np.asarray(lsi_models[t].create_embeddings(test_df[key]))\n",
    "\n",
    "elif embedding_method ==\"Doc2Vec\":\n",
    "  for dimension in dimensions:\n",
    "    matrices[dimension] = dict()\n",
    "    for t in tqdm(languages):\n",
    "      key = \"body_pre_{}\".format(t)\n",
    "      #create tagged docs first\n",
    "      documents = []\n",
    "      for ind in sample_df.index:\n",
    "        doc = sample_df[key][ind]\n",
    "        tagged_doc = TaggedDocument(doc, [ind])\n",
    "        documents.append(tagged_doc)\n",
    "      #Train Doc2Vec Model\n",
    "      model = Doc2Vec(documents, vector_size=dimension, window=3, min_count=10, workers=4, epochs=100, dm=0)\n",
    "      training_docs = [model[i] for i in train_df.index]\n",
    "      validation_docs = [model[i] for i in val_df.index]\n",
    "      test_docs = [model[i] for i in test_df.index]\n",
    "      #set matrices\n",
    "      matrices[dimension][\"{}_train_vecs\".format(t)] = np.asarray(training_docs)\n",
    "      matrices[dimension][\"{}_val_vecs\".format(t)] = np.asarray(validation_docs)\n",
    "      matrices[dimension][\"{}_test_vecs\".format(t)] = np.asarray(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JStFUUbz4fsB"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "pairs = permutations(languages, 2)\n",
    "pair_list = [p for p in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyFVUKrE0n_U",
    "outputId": "b5e750da-59a0-4413-b00f-8d9f543a37ee"
   },
   "outputs": [],
   "source": [
    "pair_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH5sFQtlxYvN"
   },
   "source": [
    "## Linear Concept Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TMIuR52vG6y"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import mate_retrieval, reciprocal_rank, comp_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwjR3Va-QAU2"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_baseline_lca_model, evaluate_baseline_lca_model_ort\n",
    "from tqdm import tqdm\n",
    "\n",
    "if test_LCA == True:\n",
    "  lca_scores = dict()\n",
    "\n",
    "  for pair in pair_list:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    if embedding_method == \"LSI\":\n",
    "      l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_test_vecs\".format(l1)]\n",
    "      l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_test_vecs\".format(l2)]\n",
    "      score_lca = evaluate_baseline_lca_model(l1_train, l1_test, l2_train, l2_test, lca_dimension, comp_scores)\n",
    "      score_lcao = evaluate_baseline_lca_model_ort(l1_train, l1_test, l2_train, l2_test, lcao_dimension, comp_scores)\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "      score_lca = []\n",
    "      score_lcao = []\n",
    "      for dimension in lca_dimension: \n",
    "        l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_test_vecs\".format(l1)]\n",
    "        l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_test_vecs\".format(l2)]\n",
    "        score_lca.append(evaluate_baseline_lca_model(l1_train, l1_test, l2_train, l2_test, [dimension], comp_scores)[0])\n",
    "      for dimension in lcao_dimension: \n",
    "        l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_test_vecs\".format(l1)]\n",
    "        l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_test_vecs\".format(l2)]\n",
    "        score_lcao.append(evaluate_baseline_lca_model_ort(l1_train, l1_test, l2_train, l2_test, [dimension], comp_scores)[0])\n",
    "\n",
    "    lca_scores[\"{}-> {}\".format(l1,l2)] = {\"lca_{}\".format(embedding_method): score_lca, \n",
    "                         \"lcao_{}\".format(embedding_method): score_lcao}\n",
    "    #Save Results\n",
    "    target_dir = main_dir+\"lca_scores_{}_{}\".format(embedding_method, dataset)\n",
    "    with open(target_dir, 'wb') as handle:\n",
    "        pickle.dump(lca_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXfoquzk6edg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXVcVDelQFbk"
   },
   "source": [
    "##LCC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uGBgYEOLsGE"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_lcc_model\n",
    "\n",
    "if test_LCC == True:\n",
    "  lcc_scores = dict()\n",
    "\n",
    "  for pair in pair_list:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    if embedding_method ==\"LSI\":\n",
    "      l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_test_vecs\".format(l1)]\n",
    "      l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_test_vecs\".format(l2)]\n",
    "      score_lcc = evaluate_lcc_model(l1_train, l1_test, l2_train, l2_test, lcc_dimension, evaluation_function = comp_scores)\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "      score_lcc = []\n",
    "      for dimension in lcc_dimension: \n",
    "        l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_test_vecs\".format(l1)]\n",
    "        l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_test_vecs\".format(l2)]\n",
    "        score_lcc.append(evaluate_lcc_model(l1_train, l1_test, l2_train, l2_test, [dimension], comp_scores)[0])\n",
    "    lcc_scores[\"{}-> {}\".format(l1,l2)] = score_lcc\n",
    "\n",
    "    #Save Results\n",
    "    target_dir = main_dir+\"lcc_scores_{}_{}\".format(embedding_method, dataset)\n",
    "    with open(target_dir, 'wb') as handle:\n",
    "        pickle.dump(lcc_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQwJAi1X_hOK"
   },
   "source": [
    "#Cross-Lingual LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNFA1i_YcbvM"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_cllsi, evaluate_improved_cllsi\n",
    "from tqdm import tqdm\n",
    "\n",
    "cllsi_scores = dict()\n",
    "if test_CLLSI == True:\n",
    "\n",
    "  for pair in tqdm(pair_list):\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    l1_train, l1_test = list(train_df[\"body_pre_{}\".format(l1)]), list(val_df[\"body_pre_{}\".format(l1)])\n",
    "    l2_train, l2_test = list(train_df[\"body_pre_{}\".format(l2)]), list(val_df[\"body_pre_{}\".format(l2)])\n",
    "    cllsi_score = evaluate_cllsi(l1_train, l1_test, l2_train, l2_test, cllsi_dimension, evaluation_function = comp_scores)\n",
    "    print(\"pair: {}, CL-LSI score: {}\".format(pair, cllsi_score) )\n",
    "    i_cllsi_score = evaluate_improved_cllsi(l1_train, l1_test, l2_train, l2_test, cllsi_dimension, evaluation_function = comp_scores)\n",
    "    print(\"pair: {}, CL-LSI score: {}\".format(pair, i_cllsi_score))\n",
    "\n",
    "    cllsi_scores[\"{}-> {}\".format(l1,l2)] = {\"cllsi_{}\".format(embedding_method): cllsi_score, \n",
    "                         \"icllsi_{}\".format(embedding_method): i_cllsi_score}\n",
    "    #Save Results\n",
    "    target_dir = main_dir+\"cllsi_scores_{}_{}\".format(embedding_method, dataset)\n",
    "    with open(target_dir, 'wb') as handle:\n",
    "        pickle.dump(lca_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNwUIz4H06vu"
   },
   "source": [
    "##Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QJd1tz-1BdZ"
   },
   "source": [
    "List all settings to be tested here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ngXuM2TqElO",
    "outputId": "4bfe1172-6899-406f-8832-0898d87dd0e4"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_nncc, evaluate_nnca\n",
    "\n",
    "if test_neural_networks == True:\n",
    "  nncc_scores = dict()\n",
    "  nnca_scores = dict()\n",
    "  #choose only one, to reduce computational burden\n",
    "  for pair in pair_list:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    \n",
    "    if embedding_method ==\"LSI\":\n",
    "        for setting in settings_nncc:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_test_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_test_vecs\".format(l2)]\n",
    "          score, history = evaluate_nncc(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = comp_scores,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          nncc_scores[\"{}-> {}\".format(l1,l2)] = score\n",
    "        for setting in settings_nnca:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_test_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_test_vecs\".format(l2)]\n",
    "          score, h1, h2 = evaluate_nnca(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = comp_scores,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          nnca_scores[\"{}-> {}\".format(l1,l2)] = score\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "        #Compute score for each nncc Setting:\n",
    "        for setting in settings_nncc:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_test_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_test_vecs\".format(l2)]\n",
    "          score, history = evaluate_nncc(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = comp_scores,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          nncc_scores[\"{}-> {}\".format(l1,l2)] = score\n",
    "        for setting in settings_nnca:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_test_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_test_vecs\".format(l2)]\n",
    "          score, h1, h2 = evaluate_nnca(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = comp_scores,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          nnca_scores[\"{}-> {}\".format(l1,l2)] = score\n",
    "\n",
    "    #Save Results\n",
    "    target_dir = main_dir+\"nnca_scores_{}_{}\".format(embedding_method, dataset)\n",
    "    with open(target_dir, 'wb') as handle:\n",
    "        pickle.dump(nnca_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    #Save Results\n",
    "    target_dir = main_dir+\"nncc_scores_{}_{}\".format(embedding_method, dataset)\n",
    "    with open(target_dir, 'wb') as handle:\n",
    "        pickle.dump(nncc_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(nncc_scores)\n",
    "\n",
    "      #lca_nn_score = evaluate_single_layer_lca_nn(l1_train, l1_test, l2_train, l2_test, evaluation_function = reciprocal_rank)\n",
    "      #lca_nn_scores.append(lca_nn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "4. TEST the Models on all Data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwECcSZvLLCB"
   },
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5qKfxEEE9E"
   },
   "source": [
    "**General Information**\n",
    "- Inside this Notebook, different methods to create multilingual document representations are tested and evaluated. \n",
    "\n",
    "- Which methods, parameter-settings and languages are used for the evaluation can be adjusted by changing the variables in the Cell below. \n",
    "\n",
    "- This Notebook was run in Google Colab. \n",
    "\n",
    "**About the Methods and Datasets**\n",
    "\n",
    "Datasets: \n",
    " - JRC-Arquis (sample of 5000 Documents)\n",
    " - EU-Bookshop (sample of ~9000 Documents, first 5000 are selected)\n",
    "\n",
    "Methods:\n",
    "\n",
    "- Methods which are based on creating mappings between monolingual corpora.\n",
    "Those methods are: Linear Concept Approximation (LCA), Linear Concept Compression(LCC) and the Neural Network versions of those: NNCA and NNCC. \n",
    "For them, first the monolingual representation have to be created, then the mapping can be applied. Algorithms which are applied here to derive monolingual representations are: Latent Semantic indexing and Doc2Vec (Mikolov et al.)\n",
    "\n",
    "- Methods which derive multilingual representations directly. Those are: Cross-Lingual Latent Semantic Indexing (CL-LSI) and the improved version of it, which is also described within the the Paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hshTz7z4DH1y"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----\n",
    "Languages Preprocessed for JRC_Arquis: en, hu, fr, de, nl, pt, cz, pl\n",
    "Languages Preprocessed for EU-Bookshop: en, es, fr\n",
    "\n",
    "\"\"\"\n",
    "#Choose either \"JRC_Arquis\" \"EU-Bookshop\"\n",
    "dataset = \"EU-Bookshop\" \n",
    "\n",
    "#Determines which methods are tested\n",
    "# True -> Method is evaluated\n",
    "# False -> Method is ignored\n",
    "test_LCA = True\n",
    "test_LCC = True\n",
    "test_CLLSI = False\n",
    "test_neural_networks = True\n",
    "\n",
    "#Set languages, dimensions and kind of monolingual embedding\n",
    "#The monolingual embedding method influences the results of \n",
    "# LCA, LCC, NNCA, and NNCC\n",
    "languages = [\"en\", \"fr\", \"es\"]\n",
    "dimensions = [50, 100, 200,  500, 800,  1600]\n",
    "embedding_method = \"Doc2Vec\"\n",
    "\n",
    "#If any other dimensions should be tested for certain models, specify here\n",
    "lca_dimensions = dimensions\n",
    "lcao_dimensions = dimensions\n",
    "lcc_dimensions = dimensions\n",
    "cllsi_dimensions = dimensions\n",
    "nn_dimensions = dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It0niRYTn2Y1"
   },
   "source": [
    "##  Load Dataset\n",
    "- First of all, clone the git repository which contains most of the functions and models for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnPUdnM5n4_H",
    "outputId": "e6614347-1cb4-4d48-bd24-6d9f6769559f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Tsegaye-misikir/NCC.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK3ve7knzwr5"
   },
   "source": [
    "- then load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elxdSt42S5Yt",
    "outputId": "2ccb2c32-08e2-4f5a-be91-05e1557f86ba"
   },
   "outputs": [],
   "source": [
    "# To run the code in the full dataset, please use train/test split\n",
    "from google.colab import drive\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "if dataset == \"JRC_Arquis\" :\n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/JRC_Arquis_files/\"\n",
    "  sample_df = pd.read_pickle(main_dir+\"sample_df_preprocessed.pkl\")\n",
    "  train_df = sample_df[:3000]\n",
    "  val_df = sample_df[3000:4000]\n",
    "  test_df = sample_df[4000:5000]\n",
    "  \n",
    "elif dataset == \"EU-Bookshop\": \n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/EU-BookShop Files/\"\n",
    "  #define\n",
    "\n",
    "  def get_eub_dataframe(main_dir):\n",
    "    def load(filepath):\n",
    "      with open(filepath,\"rb\") as f:\n",
    "          obj = pickle.load(f)\n",
    "      return obj\n",
    "    tokenized_en = load(main_dir+\"/tokenized_en.list\")\n",
    "    tokenized_fr = load(main_dir+\"/tokenized_fr.list\")\n",
    "    tokenized_es = load(main_dir+\"/tokenized_es.list\")\n",
    "    sample_df = pd.DataFrame()\n",
    "    sample_df[\"body_pre_en\"] = tokenized_en\n",
    "    sample_df[\"body_pre_fr\"] = tokenized_fr\n",
    "    sample_df[\"body_pre_es\"] = tokenized_es\n",
    "    #erase empty lists\n",
    "    for key in sample_df.keys():\n",
    "      sample_df = sample_df[sample_df.astype(str)[key] != '[]']\n",
    "    return sample_df\n",
    "\n",
    "  sample_df = get_eub_dataframe(main_dir)[:5000]\n",
    "  train_df = sample_df[:3000]\n",
    "  val_df = sample_df[3000:4000]\n",
    "  test_df = sample_df[4000:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "p6CfWmqCKfya",
    "outputId": "c849d3be-c4ea-4c6b-e735-e8180d39cfb7"
   },
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev7mVFQMvFly"
   },
   "source": [
    "## Train Monolingual Representations which will be aligned\n",
    "- > Define the languages and dimensions which should be tested here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6lpKqf-yIUG",
    "outputId": "5a6c7920-ee33-452d-913e-e0d3613a9723"
   },
   "outputs": [],
   "source": [
    "from NCC.Utils import read_docs, Vector_Lsi_Model\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm \n",
    "\n",
    "max_dim = max(dimensions)\n",
    "matrices = dict()\n",
    "\n",
    "\n",
    "if embedding_method == \"LSI\":\n",
    "  lsi_models = dict()\n",
    "  for t in languages:\n",
    "    key = \"body_pre_{}\".format(t)\n",
    "    lsi_models[t] = Vector_Lsi_Model(sample_df[key], dimension=max_dim)\n",
    "    matrices[\"{}_train_vecs\".format(t)] = np.asarray(lsi_models[t].create_embeddings(train_df[key]))\n",
    "    matrices[\"{}_val_vecs\".format(t)] = np.asarray(lsi_models[t].create_embeddings(test_df[key]))\n",
    "\n",
    "elif embedding_method ==\"Doc2Vec\":\n",
    "  for dimension in dimensions:\n",
    "    matrices[dimension] = dict()\n",
    "    for t in tqdm(languages):\n",
    "      key = \"body_pre_{}\".format(t)\n",
    "      #create tagged docs first\n",
    "      documents = []\n",
    "      for ind in sample_df.index:\n",
    "        doc = sample_df[key][ind]\n",
    "        tagged_doc = TaggedDocument(doc, [ind])\n",
    "        documents.append(tagged_doc)\n",
    "      #Train Doc2Vec Model\n",
    "      model = Doc2Vec(documents, vector_size=dimension, window=4, min_count=5, workers=4, epochs=100, dm=0)\n",
    "      training_docs = [model[i] for i in train_df.index]\n",
    "      validation_docs = [model[i] for i in val_df.index]\n",
    "      test_docs = [model[i] for i in test_df.index]\n",
    "      #set matrices\n",
    "      matrices[dimension][\"{}_train_vecs\".format(t)] = np.asarray(training_docs)\n",
    "      matrices[dimension][\"{}_val_vecs\".format(t)] = np.asarray(validation_docs)\n",
    "      matrices[dimension][\"{}_test_vecs\".format(t)] = np.asarray(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB00GRFNBxR8"
   },
   "outputs": [],
   "source": [
    "with open(main_dir+\"MATRICES__EUB\", 'wb') as handle:\n",
    "    pickle.dump(matrices, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JStFUUbz4fsB"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "pairs = permutations(languages, 2)\n",
    "pair_list = [p for p in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH5sFQtlxYvN"
   },
   "source": [
    "## Linear Concept Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QdWgvI9BmH-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "from gensim import corpora, models, matutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from gensim import models\n",
    "import itertools\n",
    "from NCC.Utils import create_corpus, read_docs, mate_retrieval_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def plot_parameter_graph(dimensions, scores, title, xlabel = \"Dimensions\", ylabel = \"Reciprocal Rank\", pair_list=None):\n",
    "  figure(figsize=(9, 6))\n",
    "\n",
    "  for k, score in enumerate(scores):\n",
    "    if pair_list == None:\n",
    "        plt.plot(dimensions, scores[k], alpha=0.8, label=\"Language Pair: {}\".format(k))\n",
    "    else:\n",
    "        plt.plot(dimensions, scores[k], alpha=0.8, label=\"Language Pair: {} -> {}\".format(pair_list[k][0], pair_list[k][1]))\n",
    "  avg = np.mean(np.asarray(scores), axis=0)\n",
    "  plt.plot(dimensions, avg, c=\"r\", label=\"Average Score\",linewidth=3.0)\n",
    "\n",
    "  max_ind = np.argmax(avg)\n",
    "\n",
    "  plt.scatter(dimensions[max_ind], avg[max_ind], c=\"k\")\n",
    "  plt.text(dimensions[max_ind], avg[max_ind]-0.03, \n",
    "          \"Dimension: {} \\nMean Score: {}\".format(dimensions[max_ind],str(avg[max_ind])[:4] ),\n",
    "          fontsize= 12\n",
    "              )\n",
    "  plt.title(title, fontsize=13)\n",
    "  plt.xlabel(\"Dimensions\")\n",
    "  plt.ylabel(\"Reciprocal Rank\")\n",
    "  #plt.ylim(0.85,1)\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "id": "pwjR3Va-QAU2",
    "outputId": "d39b27c5-7879-4a57-d8ad-e0309024bee4"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_baseline_lca_model, evaluate_baseline_lca_model_ort\n",
    "from NCC.evaluation_functions import mate_retrieval, reciprocal_rank\n",
    "from tqdm import tqdm\n",
    "\n",
    "if test_LCA == True:\n",
    "  lca_scores = []\n",
    "  lcao_scores = []\n",
    "\n",
    "  for pair in pair_list:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    if embedding_method == \"LSI\":\n",
    "      l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_val_vecs\".format(l1)]\n",
    "      l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_val_vecs\".format(l2)]\n",
    "      score_lca = evaluate_baseline_lca_model(l1_train, l1_test, l2_train, l2_test, dimensions, reciprocal_rank)\n",
    "      score_lcao = evaluate_baseline_lca_model_ort(l1_train, l1_test, l2_train, l2_test, dimensions, reciprocal_rank)\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "      score_lca = []\n",
    "      score_lcao = []\n",
    "      for dimension in dimensions: \n",
    "        l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_val_vecs\".format(l1)]\n",
    "        l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_val_vecs\".format(l2)]\n",
    "        score_lca.append(evaluate_baseline_lca_model(l1_train, l1_test, l2_train, l2_test, [dimension], reciprocal_rank)[0])\n",
    "        score_lcao.append(evaluate_baseline_lca_model_ort(l1_train, l1_test, l2_train, l2_test, [dimension], reciprocal_rank)[0])\n",
    "\n",
    "\n",
    "    lca_scores.append(score_lca)\n",
    "    lcao_scores.append(score_lcao )\n",
    "\n",
    "  \n",
    "  title_lca = \"Reciprocal Rank Scores for different dimensionality for the LSI+LCA\"\n",
    "  title_lcao = \"Reciprocal Rank Scores for different dimensionality for the LSI+LCAO\"\n",
    "\n",
    "  plot_parameter_graph(dimensions, lca_scores, title = title_lca, pair_list=pair_list)\n",
    "  plot_parameter_graph(dimensions, lcao_scores, title = title_lcao, pair_list=pair_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXVcVDelQFbk"
   },
   "source": [
    "##LCC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uGBgYEOLsGE"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_lcc_model\n",
    "\n",
    "if test_LCC == True:\n",
    "  lcc_scores = []\n",
    "\n",
    "  for pair in pair_list:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    if embedding_method ==\"LSI\":\n",
    "      l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_val_vecs\".format(l1)]\n",
    "      l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_val_vecs\".format(l2)]\n",
    "      score_lcc = evaluate_lcc_model(l1_train, l1_test, l2_train, l2_test, dimensions, evaluation_function = reciprocal_rank)\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "      score_lcc = []\n",
    "      for dimension in dimensions: \n",
    "        l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_val_vecs\".format(l1)]\n",
    "        l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_val_vecs\".format(l2)]\n",
    "        score_lcc.append(evaluate_lcc_model(l1_train, l1_test, l2_train, l2_test, [dimension], reciprocal_rank)[0])\n",
    "    lcc_scores.append(score_lcc)   \n",
    "\n",
    "  title_lcc = \"Reciprocal Rank Scores for different dimensionality for LSI+LCC\"\n",
    "  plot_parameter_graph(dimensions, lcc_scores, title = title_lcc, pair_list=pair_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "Kzz619NSJQGQ",
    "outputId": "17a8a3cb-80dc-47c5-8d7c-4d91545e2e46"
   },
   "outputs": [],
   "source": [
    "\n",
    "title_lcc = \"Reciprocal Rank Scores for different dimensionality for Doc2Vec+LCC\"\n",
    "plot_parameter_graph(dimensions, lcc_scores, title = title_lcc, pair_list=pair_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQwJAi1X_hOK"
   },
   "source": [
    "#Cross-Lingual LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "oNFA1i_YcbvM",
    "outputId": "fab08411-e45d-41e3-a4b7-85182e0038d3"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_cllsi, evaluate_improved_cllsi\n",
    "from tqdm import tqdm\n",
    "if True:\n",
    "  cllsi_scores = []\n",
    "  i_cllsi_scores = []\n",
    "\n",
    "  for pair in tqdm(pair_list):\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    l1_train, l1_test = list(train_df[\"body_pre_{}\".format(l1)]), list(val_df[\"body_pre_{}\".format(l1)])\n",
    "    l2_train, l2_test = list(train_df[\"body_pre_{}\".format(l2)]), list(val_df[\"body_pre_{}\".format(l2)])\n",
    "    cllsi_score = evaluate_cllsi(l1_train, l1_test, l2_train, l2_test, dimensions, evaluation_function = reciprocal_rank)\n",
    "    print(\"pair: {}, CL-LSI score: {}\".format(pair, cllsi_score) )\n",
    "    i_cllsi_score = evaluate_improved_cllsi(l1_train, l1_test, l2_train, l2_test, dimensions, evaluation_function = reciprocal_rank)\n",
    "    print(\"pair: {}, CL-LSI score: {}\".format(pair, i_cllsi_score))\n",
    "    cllsi_scores.append(cllsi_score)\n",
    "    i_cllsi_scores.append(i_cllsi_score)\n",
    "\n",
    "  title_cllsi = \"Reciprocal Rank Scores for different dimensionality for the CL-LSI\"\n",
    "  title_i_cllsi = \"Reciprocal Rank Scores for different dimensionality for the improved CL-LSI \"\n",
    "  plot_parameter_graph(dimensions, cllsi_scores, title = title_cllsi, pair_list=pair_list)\n",
    "  plot_parameter_graph(dimensions, i_cllsi_scores, title = title_i_cllsi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNwUIz4H06vu"
   },
   "source": [
    "##Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QJd1tz-1BdZ"
   },
   "source": [
    "List all settings to be tested here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr60p8UScPMt"
   },
   "outputs": [],
   "source": [
    "settings_nncc =  [         \n",
    "            {\"dimension\" : 1200,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"MSE\"},\n",
    "\n",
    "            {\"dimension\" : 1200,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            {\"dimension\" : 800,\n",
    "             \"neurons\" : [2000,50,2000], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            {\"dimension\" : 500,\n",
    "             \"neurons\" : [500], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "              {\"dimension\" : 500,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"}           ]     \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[#Setting 1\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"MSE\"},\n",
    "             #Setting 2\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200, 100, 200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 3\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [1000, 200, 1000], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"sgd\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 4\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.1,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            #Setting 5\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200, 100, 200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 6\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [50], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 7\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [100, 50, 100], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"}\n",
    "             ]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeuCymdopAz0"
   },
   "outputs": [],
   "source": [
    "settings_nnca = [            {\"dimension\" : 1200,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"MSE\"},\n",
    "\n",
    "            {\"dimension\" : 1200,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            {\"dimension\" : 800,\n",
    "             \"neurons\" : [2000,2000,2000], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            {\"dimension\" : 500,\n",
    "             \"neurons\" : [500], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "              {\"dimension\" : 500,\n",
    "             \"neurons\" : [300], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.0,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"}       \n",
    "\n",
    "             ]             \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "                 #Setting 1\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"MSE\"},\n",
    "             #Setting 2\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200, 100, 200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 3\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [1000, 200, 1000], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"sgd\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 4\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200], \n",
    "             \"activation_function\" : None,\n",
    "             \"dropout\" : 0.1,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "            #Setting 5\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [200, 100, 200], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 6\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [50], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"},\n",
    "             #Setting 7\n",
    "            {\"dimension\" : 200,\n",
    "             \"neurons\" : [100, 50, 100], \n",
    "             \"activation_function\" : \"relu\",\n",
    "             \"dropout\" : 0.2,\n",
    "             \"optimizer\" : \"adam\",\n",
    "             \"loss_function\" : \"cosine_sim\"}\n",
    "             ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ngXuM2TqElO"
   },
   "outputs": [],
   "source": [
    "from NCC.evaluation_functions import evaluate_nncc, evaluate_nnca\n",
    "scores = []\n",
    "if True: #test_neural_networks == True:\n",
    "  #choose only one, to reduce computational burden\n",
    "  for pair in pair_list[:1]:\n",
    "    l1 = pair[0]\n",
    "    l2 = pair[1]\n",
    "    \n",
    "    if embedding_method ==\"LSI\":\n",
    "        for setting in settings_nncc:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_val_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_val_vecs\".format(l2)]\n",
    "          score, history = evaluate_nncc(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = reciprocal_rank,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          scores.append(score[0])\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"loss\"] = history.history[\"loss\"][-1]\n",
    "          setting[\"val_loss\"] = history.history[\"val_loss\"][-1]\n",
    "          setting[\"epochs\"] = len(history.history[\"loss\"])\n",
    "        for setting in settings_nnca:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[\"{}_train_vecs\".format(l1)], matrices[\"{}_val_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[\"{}_train_vecs\".format(l2)], matrices[\"{}_val_vecs\".format(l2)]\n",
    "          score, h1, h2 = evaluate_nnca(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = reciprocal_rank,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"loss\"] = h1.history[\"loss\"][-1]\n",
    "          setting[\"val_loss\"] = h1.history[\"val_loss\"][-1]\n",
    "          setting[\"epochs\"] = len(h1.history[\"loss\"])\n",
    "    if embedding_method ==\"Doc2Vec\":\n",
    "        #Compute score for each nncc Setting:\n",
    "        for setting in settings_nncc:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_val_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_val_vecs\".format(l2)]\n",
    "          score, history = evaluate_nncc(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = reciprocal_rank,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          scores.append(score[0])\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"loss\"] = history.history[\"loss\"][-1]\n",
    "          setting[\"val_loss\"] = history.history[\"val_loss\"][-1]\n",
    "          setting[\"epochs\"] = len(history.history[\"loss\"])\n",
    "        for setting in settings_nnca:\n",
    "          dimension = setting[\"dimension\"]\n",
    "          l1_train, l1_test = matrices[dimension][\"{}_train_vecs\".format(l1)], matrices[dimension][\"{}_val_vecs\".format(l1)]\n",
    "          l2_train, l2_test = matrices[dimension][\"{}_train_vecs\".format(l2)], matrices[dimension][\"{}_val_vecs\".format(l2)]\n",
    "          score, h1, h2 = evaluate_nnca(l1_train, l1_test, l2_train, l2_test, \n",
    "                                dimensions = [dimension], \n",
    "                                evaluation_function = reciprocal_rank,\n",
    "                                neurons = setting[\"neurons\"],\n",
    "                                activation_function = setting[\"activation_function\"],\n",
    "                                max_epochs = 200,\n",
    "                                dropout = setting[\"dropout\"],\n",
    "                                optimizer = setting[\"optimizer\"],\n",
    "                                loss = setting[\"loss_function\" ])\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"score\"] = score[0]\n",
    "          setting[\"loss\"] = h1.history[\"loss\"][-1]\n",
    "          setting[\"val_loss\"] = h1.history[\"val_loss\"][-1]\n",
    "          setting[\"epochs\"] = len(h1.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2YkcxpMPed-"
   },
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKqUWsr2F5K2"
   },
   "outputs": [],
   "source": [
    "column_names = ['activation_function', 'dimension', 'dropout',  \n",
    "        'neurons','loss_function', 'optimizer', 'epochs','loss', 'val_loss','score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_VMTgVkkF8i"
   },
   "outputs": [],
   "source": [
    "nnca_df = pd.DataFrame(columns = column_names)\n",
    "for setting in settings_nnca:\n",
    "  nnca_df = nnca_df.append(setting, ignore_index= True)\n",
    "\n",
    "nncc_df = pd.DataFrame(columns = column_names)\n",
    "for setting in settings_nncc:\n",
    "  nncc_df = nncc_df.append(setting, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJgp9Gq3C20X"
   },
   "outputs": [],
   "source": [
    "nnca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAd7oKhZHH3L"
   },
   "outputs": [],
   "source": [
    "nncc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VTOMGYtkar7"
   },
   "outputs": [],
   "source": [
    "nncc_df.to_latex(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36plUjTSj5x6"
   },
   "outputs": [],
   "source": [
    "nnca_df.to_latex(index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Kopie von 3. Training Representations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

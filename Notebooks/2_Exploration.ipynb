{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It0niRYTn2Y1"
   },
   "source": [
    "##  1. Create the LSI Models/SVD for Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vfp4BHmEeqw_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAV64Q3un2Y2"
   },
   "source": [
    "The original JRC-Arquis dataset have been preprocessed before. \n",
    "The aligned documents have been saved in two seperate csv files. \n",
    "For details of the preprocessing, check: read_and_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnPUdnM5n4_H",
    "outputId": "6b39fe4a-35d3-44ab-eae3-68e62cdb05c1"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Tsegaye-misikir/NCC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Knia11EuDEsL"
   },
   "outputs": [],
   "source": [
    "dataset = \"EU-Bookshop\"\n",
    "languages = [\"es\", \"en\", \"fr\"] #[\"hu\",\"cs\",'de','en','nl',\"pl\", \"fr\", \"pt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrusNjX-ZN_Q",
    "outputId": "c3939d06-9bcb-47fe-f54c-96c1fce79b6b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "if dataset == \"JRC_Arquis\" :\n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/JRC_Arquis_files/\"\n",
    "  sample_df = pd.read_pickle(main_dir+\"sample_df_preprocessed.pkl\")\n",
    "  \n",
    "elif dataset == \"EU-Bookshop\": \n",
    "  main_dir = \"/content/gdrive/My Drive/NCC/EU-BookShop Files/\"\n",
    "  #define\n",
    "\n",
    "  def get_eub_dataframe(main_dir):\n",
    "    def load(filepath):\n",
    "      with open(filepath,\"rb\") as f:\n",
    "          obj = pickle.load(f)\n",
    "      return obj\n",
    "    tokenized_en = load(main_dir+\"/tokenized_en.list\")\n",
    "    tokenized_fr = load(main_dir+\"/tokenized_fr.list\")\n",
    "    tokenized_es = load(main_dir+\"/tokenized_es.list\")\n",
    "    sample_df = pd.DataFrame()\n",
    "    sample_df[\"body_pre_en\"] = tokenized_en\n",
    "    sample_df[\"body_pre_fr\"] = tokenized_fr\n",
    "    sample_df[\"body_pre_es\"] = tokenized_es\n",
    "    #erase empty lists\n",
    "    for key in sample_df.keys():\n",
    "      sample_df = sample_df[sample_df.astype(str)[key] != '[]']\n",
    "    return sample_df\n",
    "\n",
    "  sample_df = get_eub_dataframe(main_dir)[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sw9E5CgC7yo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDXY3YCCdKdn"
   },
   "outputs": [],
   "source": [
    "def get_frequencies(dictionary, corpus):\n",
    "  word_counts = {key:0 for key in dictionary.keys()}\n",
    "\n",
    "  for doc in corpus:\n",
    "    for word_tuple in doc:\n",
    "      index = word_tuple[0]\n",
    "      count = word_tuple[1]\n",
    "      word_counts[index] = word_counts[index]+ count\n",
    "\n",
    "  return np.sort(list(word_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pwa-ETXwPJ8-"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from NCC.Preprocessor import Preprocessor\n",
    "\n",
    "p = Preprocessor(language=\"general\")\n",
    "#Preprocessing of texts, get general statistics\n",
    "\n",
    "token_counts = []\n",
    "voc_counts = []\n",
    "corpi = []\n",
    "frequencies = []\n",
    "dictionaries = []\n",
    "for i in languages:\n",
    "  texts = sample_df[\"body_pre_\"+i]\n",
    "  dictionary = corpora.Dictionary(texts)\n",
    "  dictionaries.append(dictionary)\n",
    "  token_num = dictionary.num_pos\n",
    "  unique_words = len(dictionary.keys())\n",
    "  token_counts.append(token_num)\n",
    "  voc_counts.append(unique_words)\n",
    "  corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "  corpi.append(corpus)\n",
    "  frequencies.append(get_frequencies(dictionary, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eo3eZy_gmN8"
   },
   "source": [
    "General, descriptive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "qoZeYGQnkM6M",
    "outputId": "3efbe84d-5d66-4bb2-9c96-569c8e735b62"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# create figure and axis objects with subplots()\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "# make a plot\n",
    "ax.bar(languages, token_counts, color=\"red\")\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Language\",fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"Total Words\",color=\"red\",fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.bar(languages, voc_counts,color=\"blue\")\n",
    "ax2.set_ylabel(\"Unique Words\",color=\"blue\",fontsize=14)\n",
    "plt.title(\"{}, Total number of words and Vocabulary size\".format(dataset))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOPfhKzKETGJ"
   },
   "outputs": [],
   "source": [
    "doc_lengths = [k for k in doc_lengths if k>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFK0Yj_HFN-c",
    "outputId": "e8576b2c-1b6f-4312-953c-8e65cfe76259"
   },
   "outputs": [],
   "source": [
    "len(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "G9uK8up7ihLu",
    "outputId": "6913b4e9-894b-43b5-bde6-87a2e4f62c24"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "lin_hists =[]\n",
    "for k in range(len(languages)):\n",
    "  doc_lengths = [len(doc) for doc in corpi[k]]\n",
    "  lin_hists.append(doc_lengths)\n",
    "\n",
    "bins = np.linspace(10, 2000, 20)\n",
    "plt.hist(lin_hists, bins, label=languages)\n",
    "plt.title(\"Histogramm, tokens within {}\".format(dataset), fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "q-AGWvBIrKZ9",
    "outputId": "233d3960-541b-4965-8e6a-78118fe455e4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "hist_wl = []\n",
    "for k in range(len(dictionaries)):\n",
    "  d = dictionaries[k]\n",
    "  vocab = d.values()\n",
    "  word_lengths = [len(w) for w in vocab]\n",
    "  a = plt.hist(word_lengths, bins = np.linspace(0,30,30), alpha=0.0)\n",
    "  plt.plot(a[0]/np.sum(a[0]),  label=languages[k])\n",
    "  plt.ylim(0, 0.15)\n",
    "plt.legend()\n",
    "plt.title(\"Word Length Distribution, Vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd6EL93QXV-D"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "for k in range(len(frequencies)):\n",
    "  frequency = frequencies[k]\n",
    "\n",
    "  plt.plot(np.arange(len(frequency)),np.flip(frequency), label= languages[k])\n",
    "  plt.xlabel(\"Rank of Word\")\n",
    "  plt.ylabel(\"Frequency of Word\")\n",
    "  plt.ylim(0,1000)\n",
    "  plt.title(\"Zipfs law\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for k in range(len(frequencies)):\n",
    "  frequency = frequencies[k]\n",
    "\n",
    "  plt.plot(np.arange(len(frequency)),np.flip(frequency), label= languages[k])\n",
    "  plt.ylim(0,100)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz90mvFMtMEW"
   },
   "outputs": [],
   "source": [
    "rare_words = []\n",
    "common_words = []\n",
    "for k in range(len(frequencies)):\n",
    "  rare_words.append((frequencies[k] <= 20).sum())\n",
    "  common_words.append((frequencies[k] > 2000).sum())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "2. Exploration.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
